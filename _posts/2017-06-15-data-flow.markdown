---
layout: post
title:  "Our real-time data processing"
date:   2017-06-15 12:15:45 +0200
author: Bartosz Łoś <bartosz.los@rtbhouse.com>
image: "img/home-bg.jpg"

subtitle:    "Architecture & lessons learned."
description: "Architecture & lessons learned."
excerpt:     "Architecture & lessons learned."
---

Our platform, which takes part in auctions, purchases and emits advertisements in the Real-Time Bidding model, processes 350K bid requests and generates 30K events per every second which gives 4TB data every day. Because of machine learning, system monitoring and financial settlements we need to filter, store, aggregate and join these events together. As a result processed events and aggregated statistics are available in Hadoop, Google's BigQuery and Postgres. The most demanding are business requirements such as: events that should be joined together can appear 30 days after each other, we are not allowed to create any duplicates, we have to minimalize possible data losses as well as there could not be any differences between generated data outputs.

We have designed and implemented the solution which has reduced delay of availability of this data from 1 day to 15 seconds. It was possible because of a new approach and used technologies. It was essential to provide immutable streams of events to make it a good fit for our multi-DC architecture. Current real-time data flow in contrast to the previous solution is completely independent from bidding system which produces only light events now. Because of this separation, the core system is much more stable, but also data processing has higher quality and is easier to maintain. Additionally events making could be paused or even reprocess if it is needed.

In this post we would like to share our experience connected with scaling solution over clusters of computers in several data centers. Firstly, we would like to share a context in which we are operating on. Secondly, we will focus on our data-flow. We will go through 3 iterations, the first one, in which we were doing whole processing in core platform, the second one, in which we separed data processing from our bidding platform and the last one, in which we did real-time processing of immutable streams of events.

# Real-time bidding

When a user visits the website, we get a request from one of the ssp networks (we can think about ssp network as an ad exchange which is selling advertising space on the Internet in real-time). In response we answer if we are interested in buying advertising space giving our bid rate. Our competitors, other similar RTB companies, do the same. If we win the auction, we pay the second price and then we are able to emit our content for given cookie.

Currently we process 350K bid requests per second in peak from 30 various ssp networks. We have to answer the request in time less than 50-100 milliseconds depending on particular network. Our platform consists of two types of servlets, bidders (which process bid requests) and adservlets (which process user requests: impressions, clicks and conversions). _Impression_ is created when we emit content for given cookie, _click_ when impression is clicked and _conversion_ when user does some action which is valuable for our customers, for example when user buys something in online shop. What is most important, we pay for impressions but we earn money on those paid actions which means that we take risk to ourselves. The more optimal we buy advertising space, the more we earn. 

To be able to buy advertising space effectively, we needed to store and process data, user info and historical impressions. When it comes to user info, we wanted to know which websites he had visited and which campaigns he had seen to know what we should emit him. When it comes to impressions, we wanted to know if somebody had clicked the impression, if the conversion had occurred and how much we had earned. We were able to use this data for machine learning which meant, to put it simply, estimating probability of click or conversion. This probability was used for bid pricing.

# The first iteration

## Mutable impressions

In the first version, we kept data in [Cassandra](http://cassandra.apache.org). We had two keyspaces, the first one for mutable, historical impressions and the second one for user profiles and user clicks. Related clicks and related conversions were impression's attributes. So when the click or conversion occurred we were rewriting Cassandra's impression. The key was to find appropriate impression to modify. In case of click it was quite easy, _impression_id_ was passed in the request. In case of conversion it was a bit more complicated. Firstly, it is difficult to decide which impression or click had caused particular conversion. Secondly, it could always happen that the consumer would claim that paid action was caused not by us but by one of our competitors. To simplify, we always assign given conversion with cookie's last click. During conversion processing we were searching last click for given cookie using additional structure in Cassandra which was mapping from cookie to its previously processed clicks.

![image alt <>](/pics/df_mutable_impressions.png)

Cassandra's data was uploaded into HDFS by end-of-day MapReduce batch jobs. Because of mutable impression we were forced to download all data 30 days back and rewrite HDFS content every day. Model 
learning based on MapReduce jobs which were run on this data (mainly as Pig scripts or Hive queries).

## Drawbacks

At the beginning solution with loading data from Cassandra was good enough for us. With time, when we were growing and we were increasing volume of our data, end-of-day batch jobs lasted too long, mainly because we were rewriting all impressions 30 days back. Additionally, the events processing made logic of our servlets too complex but also in case of mistake we were almost unable to repair or reprocess it. Schemas used for data shipping was a little flexible and there was a problem that we were using various formats (we had: objects in Java code, Cassandra's columns and RCFiles on HDFS). It would be nice to have common logic for both serialization and deserialization of those to avoid mistakes connected with possible incompatibility. Finally, it would be nice to have an ability to process data using other tools than Hive and Pig (for example Crunch). We wanted to expand from one to a few DCs. Processing data locally, directly in servlets, using local Cassandra made it impossible.

In spite of machine learning we wanted to use this data to charge our clients. We wanted also to be able to do our GUIs with real-time preview. In this way we were able to monitor our campaigns. Another goal was to define campaigns' budget. If a limit was exceeded we were able to stop campaign and do not consider it during bidding and do not emit new impressions. We needed easily accessible real-time aggregates. The most obvious solution was to add logic to servlets which were counting statistics and were updating them in Postgres. These aggregates were being buffered in memory and were being upserted to Postgres in batches. However the solution with Postgres statistics led to some problems. The major one was connected with locks in Postgres which caused lags during request processing. The another one was connected with some inaccuracies caused by uncommitted stats in memory. We had also inconsistencies between aggregates and detailed events on HDFS due to the fact that they were created by two independent flows. Additionally we wanted to slim down our database and introduce the rule that servlets read it only to be able to use slave instance instead.

# The second iteration

## The first data-flow architecture

The diagram below shows high-level architecture of our first data-flow. We can see that platform was publishing messages on Kafka and those were read by two consumers, Camus which was writing raw events into HDFS and Storm which was counting real-time aggregates available almost online in Postgres. Because of loading Kafka messages from Kafka to HDFS by Camus in batches, we had a preview on raw events with 2-hour delay. The events, uploaded to HDFS, were being merged by Hive joins. So also this time impressions were being rewritten by end-of-day batch jobs and were available with 24-hour delay.

![image alt <>](/pics/df_first_dataflow.png)

## Distributed log

[Apache Kafka](https://kafka.apache.org/documentation) is distributed log which could be considered as producer-consumer queue. Because of Kafka we have achieved stable, scalable and efficient solution. Stability was achieved by Kafka replication. And scalability was achieved by an ability to add new brokers to cluster and new partitions to topics. We were able to publish and consume new types of events then, everything what we needed. Kafka holds data for given amount of time and it does not matter if data was consumed or not. It was important for us in case of consumers' temporary unavailability. Due to the fact that Kafka is stateless we were able to attach new consumers and read the same data repeatedly. We used it for storing plain events on HDFS and counting aggregates but also for many other use-cases which we came up with later. Consuming data was efficient, especially that we were reading online data (which was just produced) from system cache mainly. 

## Batch loading

Additionally we used [Camus](https://github.com/linkedin/camus) as a _Kafka to HDFS_ pipeline which was reading messages from Kafka and writing them to HDFS in our case in _2-hour batches_. Camus was dumping raw data into HDFS by map-reduce jobs. It was managing its own offsets in log files and was doing _data partitioning_ into folders on HDFS. Accordingly we had a preview on raw events with 2-hour delay.

## Avro, schema registry

We decided to use [Apache Avro](https://avro.apache.org) which is data serialization framework. This time we were storing our HDFS content in Avro files. Because of the fact that schemas were stored with it, we were able to process it later without knowing it because schema always was there.

The additional element, which we decided to add, was _Schema Registry_. We were storing historical schemas (JSON files) for Avro serialization and deserialization. Every message which we produced as byte array was send with additional header which included _schema_id_. It is how we did events versioning. The schema registry was used in bidders and adservlets (Kafka's producers), Camus (Kafka's consumer) but also as _external schema_ for Hive tables.

## Real-time, accurate statistics

Let's focus on statistics again. As mentioned before, we had some simple stats in Postgres but for various reasons they were not ideal. This time we make a decision to use [Apache Storm](http://storm.apache.org/). Apache Storm is a real-time computation system, which processes streams of tuples. Storm runs user-defined topologies which are directed graphs and consists of processing nodes (_spouts_ and _bolts_), where _spouts_ are sources of streams and emit new tuples, _bolts_ receiving tuples, do processing and emit generates tuples. Some of them could be _states_ which means that they could persist information in various data stores. Storm executes spouts and bolts as individual tasks that run in parallel on multiple machines.

It gives _fault-tolerance_ which means that in case of a failure, the worker would be relaunched and processing would be resumed from the stage where it was broken. Trident adds transactions and microbatches which help us to achieve so called _exactly-once processing_ but also good _latency and throughput balance_. 

## Stats-counter topology

We implemented stats-counter as Storm topology. Storm was reading messages from Kafka, deserializing it, counting stats and upserting state to Postgres. We were writing aggregates and Trident's _transaction_ids_ atomically. In case of rebatch we knew exactly which aggregate was updated and which one was not. It was possible because Trident assigns Kafka's offset range with _transaction_id_ which is committed in Zookeeper.

![image alt <>](/pics/df_statscounter_topology.png)

Thanks to it we have achieved so called _exactly-once state_ and accurate statistics. We set micro-batches to 15 seconds so Postgres aggregates were updated every 15 seconds. The same effect could be achieved by writing current offset with aggregates. But without transactional database or other atomic operations it would not be possible.

## Drawbacks

Unfortunately our data-flow still had some disadvantages. As mentioned, we had almost online view on aggregates and raw events with 2-hour delay but detailed, joined events were still available with 24-hour delay. Additionally Hive queries were difficult to maintain and quite ineffective for us. We were growing and our hive queries lasted too long. In case of some failure, we were forced to run it again so we get even greater delay then. The servlets' complex logic was still a problem. Because of mutable events it was also difficult to make some assumptions about data.

# The third iteration

## New approach

We wanted to have real-time processing and make the core system independent on complex data processing. The key was to introduce immutable streams of events, but it would not be possible without changing our schemas. Previously the related clicks and related conversions were impression's attributes. We changed this relation, now click contains related impression and conversion contains both previous impression and previous click. Because of this change, once created impression was immutable forever and joining could be done during click and conversion processing.

## Data-flow topology

This time we also decided to use Storm. The platform is producing light events and data-flow responsibility is to consume Kafka messages, deseralize and process them. At the end generated events are seralized and sent back to Kafka (to new topics). More precisely, processing means for us to enrich information, classify events, join them together, count some indicators but also filter events and deduplicate them. Because of huge time window (conversion could appear 30 days after impression), we needed additional storage for storing processed events to join. We decided to use a key-value storage: [Aerospike](https://www.aerospike.com). Merging algorithm is quite similar to this one which was used previously in Cassandra. This time we are storing in Aerospike: Avro objects but also some additional mappings (from cookie to previously processed events and from impression to previously processed related events) to be able to do joining operations.

![image alt <>](/pics/df_dataflow_topology.png)

We can see that now we have 3 independent topologies, one topology for one DC. It is good enough for us because we are processing independent streams of events. Now, when we are launching the second DC in the USA, we will need additional synchronization between related DCs. For example, it could happen that impression will be send from the West Coast and appropriate conversion will be send from the East Coast. Those 3 data-flows are only instances of one topology implementation, run for various topics.

What is worth mentioning, we decided to minimalize any possible dependencies on used framework, to be able to take our complex logic and run it with different computation system. Thus whole processing is done by one Storm component.

## High-level architecture

The servlets were writing messages to front Kafkas. MirrorMaker was mirroring events from three DCs to central Kafka where whole processing was done. The data-flow was working there and was sending events back to Kafka. Further we were able to count aggregates, write generated events to Google's BigQuery, HDFS or Solr using various Flume instances.

![image alt <>](/pics/df_architecture.png)

# Summary

To sum up, in the first iteration we were doing whole processing in core platform by rewriting mutable impressions in Cassandra. It was quite inflexible solution but what was even worse, we were forced to send the same data number of times from Cassanda to HDFS. In the second iteration we added Kafka and were using it for transporting our data. This time platform was producing raw events but we had different types of information with different delay. Still, we were forced to rewrite the same data on HDFS repeatedly. In the last iteration we did a real-time processing of immutable streams of events and we was streaming generated events to various sources.

In conclusion, what have we achieved exactly by those improvements? As previously mentioned, new architecture fits well with processing data from various DCs and it has already been gained by the second iteration. After the third iteration, not only have we achieved the real-time processing but this time streamed events are available almost online both in HDFS and BigQuery. HDFS's data is used mainly for our machine learning. On the other hand, we have on BigQuery easily accessible and queryable data with online view. It gives us new possibilities to monitor our platform infrastructure but also our bidding logic. Now we are able to do A/B tests for our models and we could react quickly if something is going bad. Additionally, we are able to count new types of indicators (for example how many events were deduplicated) and monitor them. Current bidding platform is completely separated from complex events processing and its business logic. Because of this separation, the core system is much more stable, but also data processing has higher quality. It was achieved partially by the second iteration but with last one data-flow is easier to develop, test and maintain. Additionally, events processing could be paused or even reprocessed  now if it is needed. Last but not least, every service in our central data-center is [dockerized](https://www.docker.com) so clusters of Kafkas, Storms, Flumes and MirrorMakers are easy to maintain and to scale them horizontally. With some custom-built tools we have achieved so-called _one-click deployment_ for our central processing infrastructure.


